{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d870fa93",
   "metadata": {},
   "source": [
    "# Model Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9792022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from xgboost import XGBClassifier, XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e34dcc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ef9ecf7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stkcd  year  province_code  city_code indcd   fixed_asset  staff  \\\n",
      "0          2  2000         440000   440300.0   K70  3.382824e+08   6616   \n",
      "1          2  2001         440000   440300.0   K70  2.883114e+08   5349   \n",
      "2          2  2002         440000   440300.0   K70  3.486585e+08   6055   \n",
      "3          2  2003         440000   440300.0   K70  2.680249e+08   7025   \n",
      "4          2  2004         440000   440300.0   K70  2.314256e+08   9627   \n",
      "...      ...   ...            ...        ...   ...           ...    ...   \n",
      "7991  900950  2006         320000   320400.0   K70  3.174713e+07    480   \n",
      "7992  900951  2006         210000   210200.0   C26  2.012465e+08   2362   \n",
      "7993  900953  2006         310000   310000.0   C36  8.829307e+08  10588   \n",
      "7994  900956  2006         420000   420200.0   C38  2.770235e+08   2951   \n",
      "7995  900957  2006         310000   310000.0   E50  2.040832e+08    104   \n",
      "\n",
      "      tfp_acf01  lninvesta   lntasset  ...  tapr_win   roa_win  leverage  age  \\\n",
      "0     10.454041  17.959246  22.449997  ...  0.068681  0.055347  0.472516   12   \n",
      "1     10.788567  17.222120  22.592436  ...  0.077412  0.058919  0.517797   13   \n",
      "2     10.681594  17.763039  22.829329  ...  0.063294  0.048449  0.582944   14   \n",
      "3     11.024557  17.541162  23.080437  ...  0.078625  0.053583  0.549243   15   \n",
      "4     11.087626  17.342579  23.466324  ...  0.081131  0.058750  0.594163   16   \n",
      "...         ...        ...        ...  ...       ...       ...       ...  ...   \n",
      "7991  12.316095  15.763618  22.439322  ...  0.081256  0.061983  0.800269    9   \n",
      "7992   9.432241  17.170340  20.331924  ... -0.235400 -0.235400  0.456506    9   \n",
      "7993   9.622582  17.482807  21.594818  ... -0.024876 -0.025135  0.650556    8   \n",
      "7994   9.765424  18.477367  21.086164  ...  0.011384  0.009073  0.684408    7   \n",
      "7995   8.785150  18.118937  20.498810  ... -0.001416 -0.001416  0.400940    8   \n",
      "\n",
      "          lnclr  unempro  unemployee  second_pro  treat_2004  post_2004  \n",
      "0     10.842165     1.45       18090   52.500000           0          0  \n",
      "1     10.894905     1.47       19378   54.060001           0          0  \n",
      "2     10.960981     1.41       19695   54.709999           0          0  \n",
      "3     10.549386     1.45       21914   59.529999           0          0  \n",
      "4     10.087483     1.58       26093   61.590000           0          1  \n",
      "...         ...      ...         ...         ...         ...        ...  \n",
      "7991  11.099542     0.92       32567   60.369999           0          1  \n",
      "7992  11.352789     1.05       60150   47.830002           1          1  \n",
      "7993  11.331293     2.03      278200   48.509998           0          1  \n",
      "7994  11.449724     1.37       34852   53.000000           0          1  \n",
      "7995  14.489648     2.03      278200   48.509998           0          1  \n",
      "\n",
      "[7996 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "panel = pd.read_csv('2004_notadd_new.csv')\n",
    "print(panel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a6f3d",
   "metadata": {},
   "source": [
    "# 2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b81fb344",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['tp_2004'] = panel['treat_2004'] * panel['post_2004']\n",
    "# make post_2004 type bool\n",
    "panel['post_2004'] = panel['post_2004'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fc445576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/pbg7pbbx5xdc686t3j5c4rkw0000gn/T/ipykernel_8130/281888229.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  panel = panel.groupby(['stkcd', 'post_2004']).mean()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_2004</th>\n",
       "      <th>year</th>\n",
       "      <th>province_code</th>\n",
       "      <th>city_code</th>\n",
       "      <th>fixed_asset</th>\n",
       "      <th>staff</th>\n",
       "      <th>tfp_acf01</th>\n",
       "      <th>lninvesta</th>\n",
       "      <th>lntasset</th>\n",
       "      <th>lntdebt</th>\n",
       "      <th>tapr_win</th>\n",
       "      <th>roa_win</th>\n",
       "      <th>leverage</th>\n",
       "      <th>age</th>\n",
       "      <th>lnclr</th>\n",
       "      <th>unempro</th>\n",
       "      <th>unemployee</th>\n",
       "      <th>second_pro</th>\n",
       "      <th>treat_2004</th>\n",
       "      <th>tp_2004</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stkcd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2001.5</td>\n",
       "      <td>440000.000000</td>\n",
       "      <td>440300.000000</td>\n",
       "      <td>3.108193e+08</td>\n",
       "      <td>6261.250000</td>\n",
       "      <td>10.737190</td>\n",
       "      <td>17.621392</td>\n",
       "      <td>22.738050</td>\n",
       "      <td>22.101366</td>\n",
       "      <td>0.072003</td>\n",
       "      <td>0.054075</td>\n",
       "      <td>0.530625</td>\n",
       "      <td>13.5</td>\n",
       "      <td>10.811859</td>\n",
       "      <td>1.445000</td>\n",
       "      <td>19769.250000</td>\n",
       "      <td>55.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>440000.000000</td>\n",
       "      <td>440300.000000</td>\n",
       "      <td>3.183829e+08</td>\n",
       "      <td>11330.000000</td>\n",
       "      <td>11.309293</td>\n",
       "      <td>17.915282</td>\n",
       "      <td>23.961760</td>\n",
       "      <td>23.479464</td>\n",
       "      <td>0.080336</td>\n",
       "      <td>0.057173</td>\n",
       "      <td>0.617797</td>\n",
       "      <td>17.0</td>\n",
       "      <td>10.173468</td>\n",
       "      <td>1.496667</td>\n",
       "      <td>27032.666667</td>\n",
       "      <td>55.746666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>2001.5</td>\n",
       "      <td>440000.000000</td>\n",
       "      <td>440300.000000</td>\n",
       "      <td>2.096603e+08</td>\n",
       "      <td>89.250000</td>\n",
       "      <td>12.323968</td>\n",
       "      <td>18.099833</td>\n",
       "      <td>22.136202</td>\n",
       "      <td>21.767839</td>\n",
       "      <td>0.006656</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.692222</td>\n",
       "      <td>12.5</td>\n",
       "      <td>14.656501</td>\n",
       "      <td>1.445000</td>\n",
       "      <td>19769.250000</td>\n",
       "      <td>55.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>440000.000000</td>\n",
       "      <td>440300.000000</td>\n",
       "      <td>1.395461e+08</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>12.063939</td>\n",
       "      <td>18.121087</td>\n",
       "      <td>21.856576</td>\n",
       "      <td>21.349926</td>\n",
       "      <td>0.048283</td>\n",
       "      <td>0.042255</td>\n",
       "      <td>0.607387</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.919624</td>\n",
       "      <td>1.496667</td>\n",
       "      <td>27032.666667</td>\n",
       "      <td>55.746666</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>2001.5</td>\n",
       "      <td>440000.000000</td>\n",
       "      <td>440300.000000</td>\n",
       "      <td>3.297029e+08</td>\n",
       "      <td>623.000000</td>\n",
       "      <td>8.823369</td>\n",
       "      <td>16.668979</td>\n",
       "      <td>20.798215</td>\n",
       "      <td>20.580685</td>\n",
       "      <td>-0.031266</td>\n",
       "      <td>-0.038605</td>\n",
       "      <td>0.805408</td>\n",
       "      <td>13.5</td>\n",
       "      <td>13.128630</td>\n",
       "      <td>1.445000</td>\n",
       "      <td>19769.250000</td>\n",
       "      <td>55.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600898</th>\n",
       "      <td>True</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>383333.333333</td>\n",
       "      <td>383433.333333</td>\n",
       "      <td>1.883967e+08</td>\n",
       "      <td>1041.000000</td>\n",
       "      <td>11.048621</td>\n",
       "      <td>13.110449</td>\n",
       "      <td>20.718652</td>\n",
       "      <td>20.303584</td>\n",
       "      <td>0.038023</td>\n",
       "      <td>0.021854</td>\n",
       "      <td>0.661559</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.108973</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>57792.000000</td>\n",
       "      <td>45.916667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600899</th>\n",
       "      <td>False</td>\n",
       "      <td>2001.5</td>\n",
       "      <td>330000.000000</td>\n",
       "      <td>330100.000000</td>\n",
       "      <td>4.206794e+07</td>\n",
       "      <td>216.750000</td>\n",
       "      <td>8.987749</td>\n",
       "      <td>12.673034</td>\n",
       "      <td>20.417642</td>\n",
       "      <td>19.814291</td>\n",
       "      <td>-0.133851</td>\n",
       "      <td>-0.130932</td>\n",
       "      <td>0.586198</td>\n",
       "      <td>5.5</td>\n",
       "      <td>12.269006</td>\n",
       "      <td>1.047500</td>\n",
       "      <td>66253.750000</td>\n",
       "      <td>51.100001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600899</th>\n",
       "      <td>True</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>330000.000000</td>\n",
       "      <td>330100.000000</td>\n",
       "      <td>1.817237e+07</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7.760179</td>\n",
       "      <td>11.101553</td>\n",
       "      <td>18.932198</td>\n",
       "      <td>19.643116</td>\n",
       "      <td>-0.524325</td>\n",
       "      <td>-0.511600</td>\n",
       "      <td>2.035860</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.672364</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>72677.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600900</th>\n",
       "      <td>False</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>110000.000000</td>\n",
       "      <td>110000.000000</td>\n",
       "      <td>2.635965e+10</td>\n",
       "      <td>1634.000000</td>\n",
       "      <td>9.468749</td>\n",
       "      <td>23.667450</td>\n",
       "      <td>24.111599</td>\n",
       "      <td>23.004606</td>\n",
       "      <td>0.072423</td>\n",
       "      <td>0.048539</td>\n",
       "      <td>0.330551</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.596313</td>\n",
       "      <td>0.610000</td>\n",
       "      <td>69589.000000</td>\n",
       "      <td>35.810001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600900</th>\n",
       "      <td>True</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>110000.000000</td>\n",
       "      <td>110000.000000</td>\n",
       "      <td>3.112481e+10</td>\n",
       "      <td>1722.333333</td>\n",
       "      <td>10.203975</td>\n",
       "      <td>20.613635</td>\n",
       "      <td>24.350276</td>\n",
       "      <td>23.410128</td>\n",
       "      <td>0.131794</td>\n",
       "      <td>0.088443</td>\n",
       "      <td>0.392687</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16.701703</td>\n",
       "      <td>0.773333</td>\n",
       "      <td>91418.333333</td>\n",
       "      <td>31.623333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2304 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_2004    year  province_code      city_code   fixed_asset  \\\n",
       "stkcd                                                                   \n",
       "2           False  2001.5  440000.000000  440300.000000  3.108193e+08   \n",
       "2            True  2005.0  440000.000000  440300.000000  3.183829e+08   \n",
       "6           False  2001.5  440000.000000  440300.000000  2.096603e+08   \n",
       "6            True  2005.0  440000.000000  440300.000000  1.395461e+08   \n",
       "7           False  2001.5  440000.000000  440300.000000  3.297029e+08   \n",
       "...           ...     ...            ...            ...           ...   \n",
       "600898       True  2005.0  383333.333333  383433.333333  1.883967e+08   \n",
       "600899      False  2001.5  330000.000000  330100.000000  4.206794e+07   \n",
       "600899       True  2004.0  330000.000000  330100.000000  1.817237e+07   \n",
       "600900      False  2003.0  110000.000000  110000.000000  2.635965e+10   \n",
       "600900       True  2005.0  110000.000000  110000.000000  3.112481e+10   \n",
       "\n",
       "               staff  tfp_acf01  lninvesta   lntasset    lntdebt  tapr_win  \\\n",
       "stkcd                                                                        \n",
       "2        6261.250000  10.737190  17.621392  22.738050  22.101366  0.072003   \n",
       "2       11330.000000  11.309293  17.915282  23.961760  23.479464  0.080336   \n",
       "6          89.250000  12.323968  18.099833  22.136202  21.767839  0.006656   \n",
       "6         117.000000  12.063939  18.121087  21.856576  21.349926  0.048283   \n",
       "7         623.000000   8.823369  16.668979  20.798215  20.580685 -0.031266   \n",
       "...              ...        ...        ...        ...        ...       ...   \n",
       "600898   1041.000000  11.048621  13.110449  20.718652  20.303584  0.038023   \n",
       "600899    216.750000   8.987749  12.673034  20.417642  19.814291 -0.133851   \n",
       "600899     57.000000   7.760179  11.101553  18.932198  19.643116 -0.524325   \n",
       "600900   1634.000000   9.468749  23.667450  24.111599  23.004606  0.072423   \n",
       "600900   1722.333333  10.203975  20.613635  24.350276  23.410128  0.131794   \n",
       "\n",
       "         roa_win  leverage   age      lnclr   unempro    unemployee  \\\n",
       "stkcd                                                                 \n",
       "2       0.054075  0.530625  13.5  10.811859  1.445000  19769.250000   \n",
       "2       0.057173  0.617797  17.0  10.173468  1.496667  27032.666667   \n",
       "6       0.002003  0.692222  12.5  14.656501  1.445000  19769.250000   \n",
       "6       0.042255  0.607387  16.0  13.919624  1.496667  27032.666667   \n",
       "7      -0.038605  0.805408  13.5  13.128630  1.445000  19769.250000   \n",
       "...          ...       ...   ...        ...       ...           ...   \n",
       "600898  0.021854  0.661559  16.0  12.108973  0.966667  57792.000000   \n",
       "600899 -0.130932  0.586198   5.5  12.269006  1.047500  66253.750000   \n",
       "600899 -0.511600  2.035860   8.0  12.672364  1.120000  72677.000000   \n",
       "600900  0.048539  0.330551   1.0  16.596313  0.610000  69589.000000   \n",
       "600900  0.088443  0.392687   3.0  16.701703  0.773333  91418.333333   \n",
       "\n",
       "        second_pro  treat_2004  tp_2004  \n",
       "stkcd                                    \n",
       "2        55.200000         0.0      0.0  \n",
       "2        55.746666         0.0      0.0  \n",
       "6        55.200000         0.0      0.0  \n",
       "6        55.746666         0.0      0.0  \n",
       "7        55.200000         0.0      0.0  \n",
       "...            ...         ...      ...  \n",
       "600898   45.916667         0.0      0.0  \n",
       "600899   51.100001         0.0      0.0  \n",
       "600899   53.000000         0.0      0.0  \n",
       "600900   35.810001         0.0      0.0  \n",
       "600900   31.623333         0.0      0.0  \n",
       "\n",
       "[2304 rows x 20 columns]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel = panel.groupby(['stkcd', 'post_2004']).mean()\n",
    "panel = panel.reset_index(level='post_2004')\n",
    "panel = panel[panel.index.duplicated(keep = False)]\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "dfa4ff6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/pbg7pbbx5xdc686t3j5c4rkw0000gn/T/ipykernel_8130/3099643869.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compact_df['Y1-Y0'] = lninvesta[panel['post_2004']] - lninvesta[~panel['post_2004']]\n"
     ]
    }
   ],
   "source": [
    "# format this in a manner sympatico with ATT estimation\n",
    "# compact_df contains only post_2004 data\n",
    "compact_df = panel[panel['post_2004']]\n",
    "\n",
    "# calcuate change in TFP\n",
    "lninvesta = panel['lninvesta'].values\n",
    "compact_df['Y1-Y0'] = lninvesta[panel['post_2004']] - lninvesta[~panel['post_2004']]\n",
    "\n",
    "# reset index so we have (1,2,3..)\n",
    "compact_df = compact_df.reset_index()\n",
    "\n",
    "# set outcome to Y1-Y0\n",
    "outcome = compact_df['Y1-Y0']\n",
    "treatment = compact_df['treat_2004'].astype(int)\n",
    "confounders = compact_df[['lntasset', 'lntdebt', 'roa_win','age', 'unempro', 'second_pro']]\n",
    "#compact_df.to_csv(\"compact.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "0f0c8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "608d5cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 1.354796242995709\n",
      "Test MSE of no-covariate model 1.832560746666037\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# TODO(victorveitch) the covariates have basically no predictive power, replace this example with something better\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "   return LinearRegression() #first model\n",
    "   #return GradientBoostingRegressor(random_state=RANDOM_SEED, n_estimators=200, max_depth=3) # second model\n",
    "   #return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=3) # third model\n",
    "   #return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=5) # fourth model\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.3)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "print(f\"Test MSE of no-covariate model {baseline_mse}\")\n",
    "   #return XGBRegressor(n_estimators=100, n_jobs=1, random_state= RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "69f4c523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE of fit model 0.22302655791621157\n",
      "Test CE of no-covariate model 0.2138855974607507\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    " return LogisticRegression(max_iter=1000) # first model\n",
    " #return GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state = RANDOM_SEED) # second model\n",
    " #return RandomForestClassifier(n_estimators=100, max_depth=3, random_state=RANDOM_SEED) # third model\n",
    " #return RandomForestClassifier(n_estimators=100, max_depth=5, random_state=RANDOM_SEED) # fourth model\n",
    "\n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.3)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_ce=log_loss(a_test, a_pred)\n",
    "print(f\"Test CE of fit model {test_ce}\") \n",
    "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "print(f\"Test CE of no-covariate model {baseline_ce}\")\n",
    "\n",
    "# return KNeighborsClassifier(n_neighbors=4)\n",
    "# return  DecisionTreeClassifier(max_depth=2, min_samples_leaf=1)\n",
    "# return XGBClassifier(n_estimators=200, n_jobs=1, nthread=None, random_state= RANDOM_SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5f64f529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q mse          1.391058\n",
       "Q baseline     1.785223\n",
       "g ce           0.189413\n",
       "g baselines    0.191816\n",
       "dtype: float64"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fit \n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "Q_mses = []\n",
    "mse_baselines = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "for train_index, test_index in kf.split(X_w_treatment, outcome):\n",
    "    X_train, X_test = X_w_treatment.loc[train_index], X_w_treatment.loc[test_index]\n",
    "    y_train, y_test = outcome.loc[train_index], outcome.loc[test_index]\n",
    "    #Q_model = Q_model_class(**Q_model_params)\n",
    "    Q_model.fit(X_train, y_train)\n",
    "    y_pred = Q_model.predict(X_test)\n",
    "    Q_mse = mean_squared_error(y_test, y_pred)\n",
    "    baseline_mse = mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "    Q_mses.append(Q_mse)\n",
    "    mse_baselines.append(baseline_mse)\n",
    "\n",
    "X = confounders.copy()\n",
    "g_ces = []\n",
    "ce_baselines = []\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "for train_index, test_index in kf.split(X, treatment):\n",
    "    X_train, X_test= X.loc[train_index], X.loc[test_index]\n",
    "    a_train, a_test = treatment.loc[train_index], treatment.loc[test_index]\n",
    "    #g_model = g_model_class(**g_model_params)\n",
    "    g_model.fit(X_train, a_train)\n",
    "    a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "    g_ce = log_loss(a_test, a_pred)\n",
    "    baseline_ce = log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "    g_ces.append(g_ce)\n",
    "    ce_baselines.append(baseline_ce)\n",
    "\n",
    "df = pd.DataFrame({'Q mse': Q_mses, 'Q baseline': mse_baselines, 'g ce': g_ces, 'g baselines': ce_baselines})\n",
    "df2 = df.mean(axis=0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0a426b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "      X_train = X.loc[train_index]\n",
    "      A_train = A.loc[train_index]\n",
    "      g = make_model()\n",
    "      g.fit(X_train, A_train)\n",
    "\n",
    "      # get predictions for split\n",
    "      predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "\n",
    "    \"\"\"\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "      kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "      kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "      X_train = X_w_treatment.loc[train_index]\n",
    "      y_train = y.loc[train_index]\n",
    "      q = make_model()\n",
    "      q.fit(X_train, y_train)\n",
    "\n",
    "      if output_type =='binary':\n",
    "        predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "        predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "      elif output_type == 'continuous':\n",
    "        predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "        predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9dc17cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Propensity Score')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)\n",
    "plt.figure()\n",
    "plt.hist(g, density=True)\n",
    "plt.title(\"Logistic Regression\")\n",
    "plt.xlabel(\"Propensity Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f272a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97b1919b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011966</td>\n",
       "      <td>1.080594</td>\n",
       "      <td>1.065755</td>\n",
       "      <td>0</td>\n",
       "      <td>0.293890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.009639</td>\n",
       "      <td>0.481930</td>\n",
       "      <td>0.481930</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.019102</td>\n",
       "      <td>-0.641106</td>\n",
       "      <td>-0.602446</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.577810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.075641</td>\n",
       "      <td>1.173264</td>\n",
       "      <td>1.159695</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.096348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.008208</td>\n",
       "      <td>0.530477</td>\n",
       "      <td>0.575057</td>\n",
       "      <td>0</td>\n",
       "      <td>0.705707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          g        Q0        Q1  A         Y\n",
       "0  0.011966  1.080594  1.065755  0  0.293890\n",
       "1  0.009639  0.481930  0.481930  0  0.021253\n",
       "2  0.019102 -0.641106 -0.602446  0 -0.577810\n",
       "3  0.075641  1.173264  1.159695  0 -0.096348\n",
       "4  0.008208  0.530477  0.575057  0  0.705707"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "data_and_nuisance_estimates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "752135f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "  \"\"\"\n",
    "  # Double ML estimator for the ATT\n",
    "  This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "  \"\"\"\n",
    "\n",
    "  if prob_t is None:\n",
    "    prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "  tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "  scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "  n = Y.shape[0] # number of observations\n",
    "  std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "  return tau_hat, std_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f6231d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is -0.25126045289509563 pm 0.3854381192130758\n"
     ]
    }
   ],
   "source": [
    "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d15fc24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( -0.6366985721081715 , 0.13417766631798017 )\n"
     ]
    }
   ],
   "source": [
    "print(\"(\", tau_hat-1.96*std_hat,\",\", tau_hat+1.96*std_hat, \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94569c2",
   "metadata": {},
   "source": [
    "### for comparison, the point estimate without any covariate correction\n",
    "outcome[treatment==1].mean()-outcome[treatment==0].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc4c8e",
   "metadata": {},
   "source": [
    "# 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "7a138513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       stkcd  year  province_code  city_code indcd   fixed_asset  staff  \\\n",
      "0          2  2000         440000   440300.0   K70  3.382824e+08   6616   \n",
      "1          2  2001         440000   440300.0   K70  2.883114e+08   5349   \n",
      "2          2  2002         440000   440300.0   K70  3.486585e+08   6055   \n",
      "3          2  2003         440000   440300.0   K70  2.680249e+08   7025   \n",
      "4          2  2004         440000   440300.0   K70  2.314256e+08   9627   \n",
      "...      ...   ...            ...        ...   ...           ...    ...   \n",
      "9872  900956  2007         420000   420200.0   C38  3.134387e+08   2851   \n",
      "9873  900956  2008         420000   420200.0   C38  5.662104e+08   2194   \n",
      "9874  900957  2006         310000   310000.0   E50  2.040832e+08    104   \n",
      "9875  900957  2007         310000   310000.0   E50  1.946251e+08    453   \n",
      "9876  900957  2008         310000   310000.0   E50  1.791904e+08    447   \n",
      "\n",
      "      tfp_acf01  lninvesta   lntasset  ...  tapr_win   roa_win  leverage  age  \\\n",
      "0     10.454041  17.959246  22.449997  ...  0.068681  0.055347  0.472516   12   \n",
      "1     10.788567  17.222120  22.592436  ...  0.077412  0.058919  0.517797   13   \n",
      "2     10.681594  17.763039  22.829329  ...  0.063294  0.048449  0.582944   14   \n",
      "3     11.024557  17.541162  23.080437  ...  0.078625  0.053583  0.549243   15   \n",
      "4     11.087626  17.342579  23.466324  ...  0.081131  0.058750  0.594163   16   \n",
      "...         ...        ...        ...  ...       ...       ...       ...  ...   \n",
      "9872  10.324251  19.333834  21.305693  ...  0.058991  0.043547  0.699585    8   \n",
      "9873  10.192242  18.687969  21.407846  ...  0.031481  0.030223  0.710362    9   \n",
      "9874   8.785150  18.118937  20.498810  ... -0.001416 -0.001416  0.400940    8   \n",
      "9875   8.713489  17.978182  20.364187  ...  0.016035  0.016035  0.375191    9   \n",
      "9876   8.802404  13.526280  20.369654  ...  0.011888  0.011888  0.301770   10   \n",
      "\n",
      "          lnclr  unempro  unemployee  second_pro  treat_2007  post_2007  \n",
      "0     10.842165     1.45       18090   52.500000           0          0  \n",
      "1     10.894905     1.47       19378   54.060001           0          0  \n",
      "2     10.960981     1.41       19695   54.709999           0          0  \n",
      "3     10.549386     1.45       21914   59.529999           0          0  \n",
      "4     10.087483     1.58       26093   61.590000           0          0  \n",
      "...         ...      ...         ...         ...         ...        ...  \n",
      "9872  11.607698     1.29       32900   53.060001           1          1  \n",
      "9873  12.460999     1.43       36723   53.450001           1          1  \n",
      "9874  14.489648     2.03      278200   48.509998           0          0  \n",
      "9875  12.970695     1.94      267800   46.590000           0          1  \n",
      "9876  12.901403     1.91      266000   45.520000           0          1  \n",
      "\n",
      "[9877 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "panel = pd.read_csv('2007_notadd_new.csv')\n",
    "print(panel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "37784165",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED=42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "2155465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel['tp_2007'] = panel['treat_2007'] * panel['post_2007']\n",
    "# make post_2004 type bool\n",
    "panel['post_2007'] = panel['post_2007'].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "0723cafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_2007</th>\n",
       "      <th>year</th>\n",
       "      <th>province_code</th>\n",
       "      <th>city_code</th>\n",
       "      <th>fixed_asset</th>\n",
       "      <th>staff</th>\n",
       "      <th>tfp_acf01</th>\n",
       "      <th>lninvesta</th>\n",
       "      <th>lntasset</th>\n",
       "      <th>lntdebt</th>\n",
       "      <th>tapr_win</th>\n",
       "      <th>roa_win</th>\n",
       "      <th>leverage</th>\n",
       "      <th>age</th>\n",
       "      <th>lnclr</th>\n",
       "      <th>unempro</th>\n",
       "      <th>unemployee</th>\n",
       "      <th>second_pro</th>\n",
       "      <th>treat_2007</th>\n",
       "      <th>tp_2007</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stkcd</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>440300.0</td>\n",
       "      <td>3.140608e+08</td>\n",
       "      <td>8433.571429</td>\n",
       "      <td>10.982377</td>\n",
       "      <td>17.747345</td>\n",
       "      <td>23.262497</td>\n",
       "      <td>22.691979</td>\n",
       "      <td>0.075574</td>\n",
       "      <td>0.055402</td>\n",
       "      <td>0.567984</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10.538263</td>\n",
       "      <td>1.467143</td>\n",
       "      <td>22882.142857</td>\n",
       "      <td>55.434285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>2007.5</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>440300.0</td>\n",
       "      <td>9.202692e+08</td>\n",
       "      <td>16489.500000</td>\n",
       "      <td>11.947134</td>\n",
       "      <td>19.277771</td>\n",
       "      <td>25.416877</td>\n",
       "      <td>25.013036</td>\n",
       "      <td>0.064683</td>\n",
       "      <td>0.046019</td>\n",
       "      <td>0.667783</td>\n",
       "      <td>19.5</td>\n",
       "      <td>10.853962</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>27070.500000</td>\n",
       "      <td>49.470001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>440300.0</td>\n",
       "      <td>1.796113e+08</td>\n",
       "      <td>101.142857</td>\n",
       "      <td>12.212527</td>\n",
       "      <td>18.108942</td>\n",
       "      <td>22.016362</td>\n",
       "      <td>21.588734</td>\n",
       "      <td>0.024496</td>\n",
       "      <td>0.019254</td>\n",
       "      <td>0.655864</td>\n",
       "      <td>14.0</td>\n",
       "      <td>14.340697</td>\n",
       "      <td>1.467143</td>\n",
       "      <td>22882.142857</td>\n",
       "      <td>55.434285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>True</td>\n",
       "      <td>2007.5</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>440300.0</td>\n",
       "      <td>3.009418e+07</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>11.961030</td>\n",
       "      <td>16.062456</td>\n",
       "      <td>22.471113</td>\n",
       "      <td>22.109813</td>\n",
       "      <td>0.039981</td>\n",
       "      <td>0.035347</td>\n",
       "      <td>0.697047</td>\n",
       "      <td>18.5</td>\n",
       "      <td>12.052963</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>27070.500000</td>\n",
       "      <td>49.470001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>440300.0</td>\n",
       "      <td>3.701775e+08</td>\n",
       "      <td>671.285714</td>\n",
       "      <td>8.773469</td>\n",
       "      <td>16.421346</td>\n",
       "      <td>20.719519</td>\n",
       "      <td>20.559153</td>\n",
       "      <td>-0.058042</td>\n",
       "      <td>-0.059039</td>\n",
       "      <td>0.854661</td>\n",
       "      <td>15.0</td>\n",
       "      <td>13.208714</td>\n",
       "      <td>1.467143</td>\n",
       "      <td>22882.142857</td>\n",
       "      <td>55.434285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900953</th>\n",
       "      <td>True</td>\n",
       "      <td>2007.5</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>8.272239e+08</td>\n",
       "      <td>10191.000000</td>\n",
       "      <td>9.873762</td>\n",
       "      <td>18.383761</td>\n",
       "      <td>21.580986</td>\n",
       "      <td>21.263036</td>\n",
       "      <td>-0.021883</td>\n",
       "      <td>-0.031474</td>\n",
       "      <td>0.727756</td>\n",
       "      <td>9.5</td>\n",
       "      <td>11.304299</td>\n",
       "      <td>1.925000</td>\n",
       "      <td>266900.000000</td>\n",
       "      <td>46.055000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900956</th>\n",
       "      <td>False</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>420200.0</td>\n",
       "      <td>2.770235e+08</td>\n",
       "      <td>2951.000000</td>\n",
       "      <td>9.765424</td>\n",
       "      <td>18.477367</td>\n",
       "      <td>21.086164</td>\n",
       "      <td>20.706963</td>\n",
       "      <td>0.011384</td>\n",
       "      <td>0.009073</td>\n",
       "      <td>0.684408</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.449724</td>\n",
       "      <td>1.370000</td>\n",
       "      <td>34852.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900956</th>\n",
       "      <td>True</td>\n",
       "      <td>2007.5</td>\n",
       "      <td>420000.0</td>\n",
       "      <td>420200.0</td>\n",
       "      <td>4.398245e+08</td>\n",
       "      <td>2522.500000</td>\n",
       "      <td>10.258247</td>\n",
       "      <td>19.010901</td>\n",
       "      <td>21.356769</td>\n",
       "      <td>21.007145</td>\n",
       "      <td>0.045236</td>\n",
       "      <td>0.036885</td>\n",
       "      <td>0.704974</td>\n",
       "      <td>8.5</td>\n",
       "      <td>12.034349</td>\n",
       "      <td>1.360000</td>\n",
       "      <td>34811.500000</td>\n",
       "      <td>53.255001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900957</th>\n",
       "      <td>False</td>\n",
       "      <td>2006.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>2.040832e+08</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>8.785150</td>\n",
       "      <td>18.118937</td>\n",
       "      <td>20.498810</td>\n",
       "      <td>19.584867</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>0.400940</td>\n",
       "      <td>8.0</td>\n",
       "      <td>14.489648</td>\n",
       "      <td>2.030000</td>\n",
       "      <td>278200.000000</td>\n",
       "      <td>48.509998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900957</th>\n",
       "      <td>True</td>\n",
       "      <td>2007.5</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>310000.0</td>\n",
       "      <td>1.869078e+08</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>8.757946</td>\n",
       "      <td>15.752231</td>\n",
       "      <td>20.366920</td>\n",
       "      <td>19.277717</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.013961</td>\n",
       "      <td>0.338481</td>\n",
       "      <td>9.5</td>\n",
       "      <td>12.936049</td>\n",
       "      <td>1.925000</td>\n",
       "      <td>266900.000000</td>\n",
       "      <td>46.055000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2416 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        post_2007    year  province_code  city_code   fixed_asset  \\\n",
       "stkcd                                                               \n",
       "2           False  2003.0       440000.0   440300.0  3.140608e+08   \n",
       "2            True  2007.5       440000.0   440300.0  9.202692e+08   \n",
       "6           False  2003.0       440000.0   440300.0  1.796113e+08   \n",
       "6            True  2007.5       440000.0   440300.0  3.009418e+07   \n",
       "7           False  2003.0       440000.0   440300.0  3.701775e+08   \n",
       "...           ...     ...            ...        ...           ...   \n",
       "900953       True  2007.5       310000.0   310000.0  8.272239e+08   \n",
       "900956      False  2006.0       420000.0   420200.0  2.770235e+08   \n",
       "900956       True  2007.5       420000.0   420200.0  4.398245e+08   \n",
       "900957      False  2006.0       310000.0   310000.0  2.040832e+08   \n",
       "900957       True  2007.5       310000.0   310000.0  1.869078e+08   \n",
       "\n",
       "               staff  tfp_acf01  lninvesta   lntasset    lntdebt  tapr_win  \\\n",
       "stkcd                                                                        \n",
       "2        8433.571429  10.982377  17.747345  23.262497  22.691979  0.075574   \n",
       "2       16489.500000  11.947134  19.277771  25.416877  25.013036  0.064683   \n",
       "6         101.142857  12.212527  18.108942  22.016362  21.588734  0.024496   \n",
       "6         175.000000  11.961030  16.062456  22.471113  22.109813  0.039981   \n",
       "7         671.285714   8.773469  16.421346  20.719519  20.559153 -0.058042   \n",
       "...              ...        ...        ...        ...        ...       ...   \n",
       "900953  10191.000000   9.873762  18.383761  21.580986  21.263036 -0.021883   \n",
       "900956   2951.000000   9.765424  18.477367  21.086164  20.706963  0.011384   \n",
       "900956   2522.500000  10.258247  19.010901  21.356769  21.007145  0.045236   \n",
       "900957    104.000000   8.785150  18.118937  20.498810  19.584867 -0.001416   \n",
       "900957    450.000000   8.757946  15.752231  20.366920  19.277717  0.013961   \n",
       "\n",
       "         roa_win  leverage   age      lnclr   unempro     unemployee  \\\n",
       "stkcd                                                                  \n",
       "2       0.055402  0.567984  15.0  10.538263  1.467143   22882.142857   \n",
       "2       0.046019  0.667783  19.5  10.853962  1.230000   27070.500000   \n",
       "6       0.019254  0.655864  14.0  14.340697  1.467143   22882.142857   \n",
       "6       0.035347  0.697047  18.5  12.052963  1.230000   27070.500000   \n",
       "7      -0.059039  0.854661  15.0  13.208714  1.467143   22882.142857   \n",
       "...          ...       ...   ...        ...       ...            ...   \n",
       "900953 -0.031474  0.727756   9.5  11.304299  1.925000  266900.000000   \n",
       "900956  0.009073  0.684408   7.0  11.449724  1.370000   34852.000000   \n",
       "900956  0.036885  0.704974   8.5  12.034349  1.360000   34811.500000   \n",
       "900957 -0.001416  0.400940   8.0  14.489648  2.030000  278200.000000   \n",
       "900957  0.013961  0.338481   9.5  12.936049  1.925000  266900.000000   \n",
       "\n",
       "        second_pro  treat_2007  tp_2007  \n",
       "stkcd                                    \n",
       "2        55.434285         0.0      0.0  \n",
       "2        49.470001         0.0      0.0  \n",
       "6        55.434285         0.0      0.0  \n",
       "6        49.470001         0.0      0.0  \n",
       "7        55.434285         0.0      0.0  \n",
       "...            ...         ...      ...  \n",
       "900953   46.055000         0.0      0.0  \n",
       "900956   53.000000         1.0      0.0  \n",
       "900956   53.255001         1.0      1.0  \n",
       "900957   48.509998         0.0      0.0  \n",
       "900957   46.055000         0.0      0.0  \n",
       "\n",
       "[2416 rows x 20 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panel = panel.groupby(['stkcd', 'post_2007']).mean()\n",
    "panel = panel.reset_index(level='post_2007')\n",
    "panel = panel[panel.index.duplicated(keep = False)]\n",
    "panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6a879be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jr/pbg7pbbx5xdc686t3j5c4rkw0000gn/T/ipykernel_8130/3041782888.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  compact_df['Y1-Y0'] = lninvest[panel['post_2007']] - lninvest[~panel['post_2007']]\n"
     ]
    }
   ],
   "source": [
    "# format this in a manner sympatico with ATT estimation\n",
    "# compact_df contains only post_2004 data\n",
    "compact_df = panel[panel['post_2007']]\n",
    "\n",
    "# calcuate change in TFP\n",
    "lninvest = panel['lninvesta'].values\n",
    "compact_df['Y1-Y0'] = lninvest[panel['post_2007']] - lninvest[~panel['post_2007']]\n",
    "\n",
    "# reset index so we have (1,2,3..)\n",
    "compact_df = compact_df.reset_index()\n",
    "\n",
    "# set outcome to Y1-Y0\n",
    "outcome = compact_df['Y1-Y0']\n",
    "treatment = compact_df['treat_2007'].astype(int)\n",
    "confounders = compact_df[['lntasset', 'lntdebt', 'roa_win','age', 'unempro', 'second_pro']]\n",
    "#compact_df.to_csv(\"compact.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "2c013ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE of fit model 1.6681059805481935\n",
      "Test MSE of no-covariate model 2.398295711024184\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the conditional expected outcome\n",
    "\n",
    "# TODO(victorveitch) the covariates have basically no predictive power, replace this example with something better\n",
    "\n",
    "# make a function that returns a sklearn model for later use in k-folding\n",
    "def make_Q_model():\n",
    "   #return LinearRegression() # first model\n",
    "   #return GradientBoostingRegressor(random_state=RANDOM_SEED, n_estimators=300, max_depth=3) # second model \n",
    "   #return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=3) # third model\n",
    "   return RandomForestRegressor(random_state=RANDOM_SEED, n_estimators=100, max_depth=5) #acceptable\n",
    "   # return XGBRegressor(random_state=RANDOM_SEED, n_estimators=200, n_jobs=1)\n",
    "Q_model = make_Q_model()\n",
    "\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_w_treatment, outcome, test_size=0.2)\n",
    "Q_model.fit(X_train, y_train)\n",
    "y_pred = Q_model.predict(X_test)\n",
    "\n",
    "test_mse=mean_squared_error(y_pred, y_test)\n",
    "print(f\"Test MSE of fit model {test_mse}\") \n",
    "baseline_mse=mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "print(f\"Test MSE of no-covariate model {baseline_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "65cbb123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CE of fit model 0.2558908425682924\n",
      "Test CE of no-covariate model 0.34589133900921565\n"
     ]
    }
   ],
   "source": [
    "# specify a model for the propensity score\n",
    "\n",
    "def make_g_model():\n",
    " \n",
    " #return LogisticRegression(max_iter=1000) # first model\n",
    " #return GradientBoostingClassifier(n_estimators=300, max_depth=1, random_state = RANDOM_SEED) # second model\n",
    " #return RandomForestClassifier(n_estimators=100, max_depth=3) # third model \n",
    " return RandomForestClassifier(n_estimators=100, max_depth=5) # fourth model\n",
    "\n",
    " # return XGBClassifier(n_estimators=100, n_jobs=1, nthread=None, random_state= RANDOM_SEED) #unacceptable \n",
    "\n",
    "g_model = make_g_model()\n",
    "# Sanity check that chosen model actually improves test error\n",
    "# A real analysis should give substantial attention to model selection and validation \n",
    "\n",
    "X_train, X_test, a_train, a_test = train_test_split(confounders, treatment, test_size=0.2)\n",
    "g_model.fit(X_train, a_train)\n",
    "a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "test_ce=log_loss(a_test, a_pred)\n",
    "print(f\"Test CE of fit model {test_ce}\") \n",
    "baseline_ce=log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "print(f\"Test CE of no-covariate model {baseline_ce}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8c175d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Q mse          2.428246\n",
       "Q baseline     3.008976\n",
       "g ce           0.220827\n",
       "g baselines    0.291447\n",
       "dtype: float64"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model fit \n",
    "X_w_treatment = confounders.copy()\n",
    "X_w_treatment[\"treatment\"] = treatment\n",
    "\n",
    "Q_mses = []\n",
    "mse_baselines = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "for train_index, test_index in kf.split(X_w_treatment, outcome):\n",
    "    X_train, X_test = X_w_treatment.loc[train_index], X_w_treatment.loc[test_index]\n",
    "    y_train, y_test = outcome.loc[train_index], outcome.loc[test_index]\n",
    "    #Q_model = Q_model_class(**Q_model_params)\n",
    "    Q_model.fit(X_train, y_train)\n",
    "    y_pred = Q_model.predict(X_test)\n",
    "    Q_mse = mean_squared_error(y_test, y_pred)\n",
    "    baseline_mse = mean_squared_error(y_train.mean()*np.ones_like(y_test), y_test)\n",
    "    Q_mses.append(Q_mse)\n",
    "    mse_baselines.append(baseline_mse)\n",
    "\n",
    "X = confounders.copy()\n",
    "g_ces = []\n",
    "ce_baselines = []\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "for train_index, test_index in kf.split(X, treatment):\n",
    "    X_train, X_test= X.loc[train_index], X.loc[test_index]\n",
    "    a_train, a_test = treatment.loc[train_index], treatment.loc[test_index]\n",
    "    #g_model = g_model_class(**g_model_params)\n",
    "    g_model.fit(X_train, a_train)\n",
    "    a_pred = g_model.predict_proba(X_test)[:,1]\n",
    "    g_ce = log_loss(a_test, a_pred)\n",
    "    baseline_ce = log_loss(a_test, a_train.mean()*np.ones_like(a_test))\n",
    "    g_ces.append(g_ce)\n",
    "    ce_baselines.append(baseline_ce)\n",
    "\n",
    "df = pd.DataFrame({'Q mse': Q_mses, 'Q baseline': mse_baselines, 'g ce': g_ces, 'g baselines': ce_baselines})\n",
    "df2 = df.mean(axis=0)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "9ddf7a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_k_fold_fit_and_predict(make_model, X:pd.DataFrame, A:np.array, n_splits:int):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the treatment A. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns an array containing the predictions  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (which implements fit and predict_prob)\n",
    "    X: dataframe of variables to adjust for\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    \"\"\"\n",
    "    predictions = np.full_like(A, np.nan, dtype=float)\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X, A):\n",
    "      X_train = X.loc[train_index]\n",
    "      A_train = A.loc[train_index]\n",
    "      g = make_model()\n",
    "      g.fit(X_train, A_train)\n",
    "\n",
    "      # get predictions for split\n",
    "      predictions[test_index] = g.predict_proba(X.loc[test_index])[:, 1]\n",
    "\n",
    "    assert np.isnan(predictions).sum() == 0\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def outcome_k_fold_fit_and_predict(make_model, X:pd.DataFrame, y:np.array, A:np.array, n_splits:int, output_type:str):\n",
    "    \"\"\"\n",
    "    Implements K fold cross-fitting for the model predicting the outcome Y. \n",
    "    That is, \n",
    "    1. Split data into K folds\n",
    "    2. For each fold j, the model is fit on the other K-1 folds\n",
    "    3. The fitted model is used to make predictions for each data point in fold j\n",
    "    Returns two arrays containing the predictions for all units untreated, all units treated  \n",
    "\n",
    "    Args:\n",
    "    model: function that returns sklearn model (that implements fit and either predict_prob or predict)\n",
    "    X: dataframe of variables to adjust for\n",
    "    y: array of outcomes\n",
    "    A: array of treatments\n",
    "    n_splits: number of splits to use\n",
    "    output_type: type of outcome, \"binary\" or \"continuous\"\n",
    "\n",
    "    \"\"\"\n",
    "    predictions0 = np.full_like(A, np.nan, dtype=float)\n",
    "    predictions1 = np.full_like(y, np.nan, dtype=float)\n",
    "    if output_type == 'binary':\n",
    "      kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "    elif output_type == 'continuous':\n",
    "      kf = KFold(n_splits=n_splits, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    # include the treatment as input feature\n",
    "    X_w_treatment = X.copy()\n",
    "    X_w_treatment[\"A\"] = A\n",
    "\n",
    "    # for predicting effect under treatment / control status for each data point \n",
    "    X0 = X_w_treatment.copy()\n",
    "    X0[\"A\"] = 0\n",
    "    X1 = X_w_treatment.copy()\n",
    "    X1[\"A\"] = 1\n",
    "\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_w_treatment, y):\n",
    "      X_train = X_w_treatment.loc[train_index]\n",
    "      y_train = y.loc[train_index]\n",
    "      q = make_model()\n",
    "      q.fit(X_train, y_train)\n",
    "\n",
    "      if output_type =='binary':\n",
    "        predictions0[test_index] = q.predict_proba(X0.loc[test_index])[:, 1]\n",
    "        predictions1[test_index] = q.predict_proba(X1.loc[test_index])[:, 1]\n",
    "      elif output_type == 'continuous':\n",
    "        predictions0[test_index] = q.predict(X0.loc[test_index])\n",
    "        predictions1[test_index] = q.predict(X1.loc[test_index])\n",
    "\n",
    "    assert np.isnan(predictions0).sum() == 0\n",
    "    assert np.isnan(predictions1).sum() == 0\n",
    "    return predictions0, predictions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "344a8773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Propensity Score')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = treatment_k_fold_fit_and_predict(make_g_model, X=confounders, A=treatment, n_splits=10)\n",
    "plt.figure()\n",
    "plt.hist(g, density=True)\n",
    "plt.title(\"Random Forest (depth 5)\")\n",
    "plt.xlabel(\"Propensity Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4fa3708f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q0,Q1=outcome_k_fold_fit_and_predict(make_Q_model, X=confounders, y=outcome, A=treatment, n_splits=10, output_type=\"continuous\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c3877ca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>Q0</th>\n",
       "      <th>Q1</th>\n",
       "      <th>A</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.243715</td>\n",
       "      <td>0.933795</td>\n",
       "      <td>0.963456</td>\n",
       "      <td>0</td>\n",
       "      <td>1.530426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.047100</td>\n",
       "      <td>0.949235</td>\n",
       "      <td>0.957190</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.046486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.046245</td>\n",
       "      <td>-0.467394</td>\n",
       "      <td>-0.467394</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.940513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.031240</td>\n",
       "      <td>-4.211562</td>\n",
       "      <td>-4.211562</td>\n",
       "      <td>0</td>\n",
       "      <td>0.521937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054108</td>\n",
       "      <td>0.941816</td>\n",
       "      <td>0.948853</td>\n",
       "      <td>0</td>\n",
       "      <td>0.745998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          g        Q0        Q1  A         Y\n",
       "0  0.243715  0.933795  0.963456  0  1.530426\n",
       "1  0.047100  0.949235  0.957190  0 -2.046486\n",
       "2  0.046245 -0.467394 -0.467394  0 -0.940513\n",
       "3  0.031240 -4.211562 -4.211562  0  0.521937\n",
       "4  0.054108  0.941816  0.948853  0  0.745998"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_and_nuisance_estimates = pd.DataFrame({'g': g, 'Q0': Q0, 'Q1': Q1, 'A': treatment, 'Y': outcome})\n",
    "data_and_nuisance_estimates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b9bf0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def att_aiptw(Q0, Q1, g, A, Y, prob_t=None):\n",
    "  \"\"\"\n",
    "  # Double ML estimator for the ATT\n",
    "  This uses the ATT specific scores, see equation 3.9 of https://www.econstor.eu/bitstream/10419/149795/1/869216953.pdf\n",
    "  \"\"\"\n",
    "\n",
    "  if prob_t is None:\n",
    "    prob_t = A.mean() # estimate marginal probability of treatment\n",
    "\n",
    "  tau_hat = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0)).mean()/ prob_t\n",
    "  \n",
    "  scores = (A*(Y-Q0) - (1-A)*(g/(1-g))*(Y-Q0) - tau_hat*A) / prob_t\n",
    "  n = Y.shape[0] # number of observations\n",
    "  std_hat = np.std(scores) / np.sqrt(n)\n",
    "\n",
    "  return tau_hat, std_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7efc9d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The estimate is 0.3320429405874762 pm 0.2369106597530061\n"
     ]
    }
   ],
   "source": [
    "tau_hat, std_hat = att_aiptw(**data_and_nuisance_estimates)\n",
    "print(f\"The estimate is {tau_hat} pm {1.96*std_hat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c420a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( 0.09513228083447009 , 0.5689536003404823 )\n"
     ]
    }
   ],
   "source": [
    "print(\"(\", tau_hat-1.96*std_hat,\",\", tau_hat+1.96*std_hat, \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8bff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
